{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from seq2seq import Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from seq2seq import Seq2seq\n",
    "from data_handler import Data\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "# Model related\n",
    "tf.flags.DEFINE_integer('num_units'         , 256           , 'Number of units in a LSTM cell')\n",
    "tf.flags.DEFINE_integer('embed_dim'         , 256           , 'Size of the embedding vector')\n",
    "\n",
    "# Training related\n",
    "tf.flags.DEFINE_float('learning_rate'       , 0.001         , 'learning rate for the optimizer')\n",
    "tf.flags.DEFINE_string('optimizer'          , 'Adam'        , 'Name of the train source file')\n",
    "tf.flags.DEFINE_integer('batch_size'        , 32            , 'random seed for training sampling')\n",
    "tf.flags.DEFINE_integer('print_every'       , 100           , 'print records every n iteration')\n",
    "tf.flags.DEFINE_integer('iterations'        , 10000         , 'number of iterations to train')\n",
    "tf.flags.DEFINE_string('model_dir'          , 'checkpoints_new' , 'Directory where to save the model')\n",
    "tf.flags.DEFINE_string('experiment_dir'          , 'experiments' , 'Directory where to save the experiment')\n",
    "\n",
    "tf.flags.DEFINE_integer('input_max_length'  , 30            , 'Max length of input sequence to use')\n",
    "tf.flags.DEFINE_integer('output_max_length' , 30            , 'Max length of output sequence to use')\n",
    "tf.flags.DEFINE_integer('max_length' , 30            , 'Max length of output sequence to use')\n",
    "\n",
    "tf.flags.DEFINE_bool('use_residual_lstm'    , True          , 'To use the residual connection with the residual LSTM')\n",
    "\n",
    "# Data related\n",
    "tf.flags.DEFINE_string('input_filename', 'data/mscoco/train_source.txt', 'Name of the train source file')\n",
    "tf.flags.DEFINE_string('output_filename', 'data/mscoco/train_target.txt', 'Name of the train target file')\n",
    "tf.flags.DEFINE_string('vocab_filename', 'data/mscoco/train_vocab.txt', 'Name of the vocab file')\n",
    "tf.flags.DEFINE_string('shuffled_filename', 'data/mscoco/train_target_shuffled.txt', 'Name of shuffled targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn import learn_runner\n",
    "\n",
    "def run_experiment(argv=None):\n",
    "    \n",
    "    run_config = tf.contrib.learn.RunConfig()\n",
    "    run_config = run_config.replace(model_dir=FLAGS.experiment_dir)\n",
    "    \n",
    "    learn_runner.run(experiment_fn=experiment_fn,\n",
    "                     run_config=run_config,\n",
    "                     schedule='train_and_evaluate'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_fn(run_config, params):\n",
    "    data = Data(FLAGS)\n",
    "\n",
    "    model = Seq2seq(data.vocab_size, FLAGS)\n",
    "    estimator = tf.estimator.Estimator(model_fn=model.make_graph, \n",
    "#                                        model_dir=FLAGS.model_dir, \n",
    "                                       config=run_config,\n",
    "                                       params=FLAGS)\n",
    "    \n",
    "    train_input_fn, train_feed_fn = data.make_input_fn('train')\n",
    "    eval_input_fn, eval_feed_fn = data.make_input_fn('test')\n",
    "    \n",
    "    print_vars = [\n",
    "        'source_ex',\n",
    "        'target_ex',\n",
    "        'predict'\n",
    "        # 'decoder_output',\n",
    "        # 'actual'\n",
    "    ]\n",
    "    print_inputs = tf.train.LoggingTensorHook(print_vars ,\n",
    "                                              every_n_iter=FLAGS.print_every,\n",
    "                                              formatter=data.get_formatter(['source_ex', 'target_ex', 'predict']))\n",
    "\n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        estimator=estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=eval_input_fn,\n",
    "        train_steps=FLAGS.iterations,\n",
    "        min_eval_frequency=FLAGS.print_every,\n",
    "        train_monitors=[tf.train.FeedFnHook(train_feed_fn), print_inputs],\n",
    "        eval_hooks=[tf.train.FeedFnHook(eval_feed_fn)],\n",
    "        eval_steps=10\n",
    "    )\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1198f8e90>, '_model_dir': 'experiments', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_evaluation_master': '', '_master': ''}\n",
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /Users/zalexander/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into experiments/model.ckpt.\n",
      "INFO:tensorflow:\n",
      "****source_ex == a very clean and well decorated empty bathroom           \n",
      "****target_ex == a young man is riding on a horse and enjoying her time       \n",
      "****predict == thirteen grins stardust gymnasium deserted 'no booed checkpoint snickers paired prague panchetta umpires moan order:svu talks lit located located looks located the located work work work it sink tiled train \n",
      "INFO:tensorflow:loss = 20.7729, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-15:04:51\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-15:04:57\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 20.7688, sim_loss = 0.691874, source_loss = 10.0385, target_loss = 10.0384\n",
      "INFO:tensorflow:Validation (step 100): source_loss = 10.0385, loss = 20.7688, target_loss = 10.0384, sim_loss = 0.691874, global_step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.84241\n",
      "INFO:tensorflow:\n",
      "****source_ex == there is a sparse kitchen with no table or chairs                  \n",
      "****target_ex == a clean and empty looking kitchen with a great tiled floor                 \n",
      "****predict == a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "INFO:tensorflow:loss = 12.0122, step = 101 (118.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.915954\n",
      "INFO:tensorflow:\n",
      "****source_ex == a kitchen filled with pots and pants next to a counter             \n",
      "****target_ex == a white truck driving down a curvy road                \n",
      "****predict == a a a a a a a \n",
      "INFO:tensorflow:loss = 11.3534, step = 201 (109.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.993453\n",
      "INFO:tensorflow:\n",
      "****source_ex == a cookie sheet with red sliced tomatoes and a platter of whole tomatoes on a crowded kitchen counter \n",
      "****target_ex == a kitchen counter top with a tray of sliced tomatoes and a plate of whole tomatoes \n",
      "****predict == a bathroom with a bathroom   \n",
      "INFO:tensorflow:loss = 10.4336, step = 301 (100.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.973477\n",
      "INFO:tensorflow:\n",
      "****source_ex == woman and man around kitchen island with grocery bags and beverages         \n",
      "****target_ex == young people and their groceries in a kitchen      \n",
      "****predict == a bathroom with a bathroom with a bathroom \n",
      "INFO:tensorflow:loss = 9.52617, step = 401 (102.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.985835\n",
      "INFO:tensorflow:\n",
      "****source_ex == there is a fancy bathroom with tile floors a toilet a sink and a bathtub     \n",
      "****target_ex == a modern bathroom including toilet basin and tub and shower enclosure         \n",
      "****predict == a bathroom with a bathroom with a bathroom  \n",
      "INFO:tensorflow:loss = 9.29747, step = 501 (101.437 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 563 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-15:13:46\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-563\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-15:13:52\n",
      "INFO:tensorflow:Saving dict for global step 563: global_step = 563, loss = 10.0999, sim_loss = 0.614175, source_loss = 4.78283, target_loss = 4.70287\n",
      "INFO:tensorflow:Validation (step 600): source_loss = 4.78283, loss = 10.0999, target_loss = 4.70287, sim_loss = 0.614175, global_step = 563\n",
      "INFO:tensorflow:global_step/sec: 0.871753\n",
      "INFO:tensorflow:\n",
      "****source_ex == there are many people outside having some fun       \n",
      "****target_ex == a young boy and girl ride a bicycle together       \n",
      "****predict == a man in a kitchen with a street  \n",
      "INFO:tensorflow:loss = 9.63475, step = 601 (114.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.972671\n",
      "INFO:tensorflow:\n",
      "****source_ex == a kitchen with many pots and pans hanging above the oven     \n",
      "****target_ex == there are many pots that are hanging over the stove        \n",
      "****predict == a bathroom with a bathroom with a bathroom     \n",
      "INFO:tensorflow:loss = 8.1771, step = 701 (102.809 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.974571\n",
      "INFO:tensorflow:\n",
      "****source_ex == a red double decker bus with a bicycle loaded with gear parked next to it   \n",
      "****target_ex == a catcher reaching out to catch a ball while the batter is swinging     \n",
      "****predict == a man is on a bike with a street \n",
      "INFO:tensorflow:loss = 7.98363, step = 801 (102.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.00109\n",
      "INFO:tensorflow:\n",
      "****source_ex == this is an image of a blue door and window on a building    \n",
      "****target_ex == a group of zebra walking on top of a lush green field        \n",
      "****predict == a man is on a motorcycle with a motorcycle    \n",
      "INFO:tensorflow:loss = 9.59172, step = 901 (99.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.995444\n",
      "INFO:tensorflow:\n",
      "****source_ex == a photo of a laptop on a bed with a tv in the background     \n",
      "****target_ex == man sitting cross legged on bed in room           \n",
      "****predict == a man is on a motorcycle with a street \n",
      "INFO:tensorflow:loss = 8.65106, step = 1001 (100.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.984825\n",
      "INFO:tensorflow:\n",
      "****source_ex == a person stopped at the intersection of a street on a motorcycle   \n",
      "****target_ex == a street scene of a person sitting on a motorcycle        \n",
      "****predict == a man is on a motorcycle on a motorcycle \n",
      "INFO:tensorflow:loss = 9.76376, step = 1101 (101.542 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1142 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-15:24:08\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-1142\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-15:24:15\n",
      "INFO:tensorflow:Saving dict for global step 1142: global_step = 1142, loss = 8.97439, sim_loss = 0.567639, source_loss = 4.1753, target_loss = 4.23146\n",
      "INFO:tensorflow:Validation (step 1200): source_loss = 4.1753, loss = 8.97439, target_loss = 4.23146, sim_loss = 0.567639, global_step = 1142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.868234\n",
      "INFO:tensorflow:\n",
      "****source_ex == rows of motorcycles are on either side of the street      \n",
      "****target_ex == many people and line of parked scooters and motorcycles at night     \n",
      "****predict == a man is on a motorcycle with a motorcycle   \n",
      "INFO:tensorflow:loss = 8.94155, step = 1201 (115.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.99885\n",
      "INFO:tensorflow:\n",
      "****source_ex == a motorcycle with a bag on the back of it parked in the road  \n",
      "****target_ex == a red stop sign sitting in the middle of a tall grass covered field  \n",
      "****predict == a man is sitting on a motorcycle with a motorcycle \n",
      "INFO:tensorflow:loss = 7.94886, step = 1301 (100.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.981491\n",
      "INFO:tensorflow:\n",
      "****source_ex == the small motorcycles are lined up on the grass                    \n",
      "****target_ex == this is an image of a row of motorcycles         \n",
      "****predict == a man sitting on a motorcycle       \n",
      "INFO:tensorflow:loss = 8.52021, step = 1401 (101.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.996432\n",
      "INFO:tensorflow:\n",
      "****source_ex == a jet sits on a tarmac with airport terminals in the distance    \n",
      "****target_ex == a large jet airplane resting on the ground at a runway airport    \n",
      "****predict == a large airplane flying through the sky     \n",
      "INFO:tensorflow:loss = 7.88991, step = 1501 (100.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.00375\n",
      "INFO:tensorflow:\n",
      "****source_ex == this blue tray is holding a variety of foods          \n",
      "****target_ex == food displayed partially eaten on a table for little ones    \n",
      "****predict == a man sitting on a street    \n",
      "INFO:tensorflow:loss = 8.15181, step = 1601 (99.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.990914\n",
      "INFO:tensorflow:\n",
      "****source_ex == a colorful hydrant sitting in the grass on a quiet street      \n",
      "****target_ex == a close of a fire hydrant painted red white and green     \n",
      "****predict == a man is sitting on a bench    \n",
      "INFO:tensorflow:loss = 8.19003, step = 1701 (100.917 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1725 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-15:34:29\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-1725\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-15:34:35\n",
      "INFO:tensorflow:Saving dict for global step 1725: global_step = 1725, loss = 8.12165, sim_loss = 0.561971, source_loss = 3.74494, target_loss = 3.81474\n",
      "INFO:tensorflow:Validation (step 1800): source_loss = 3.74494, loss = 8.12165, target_loss = 3.81474, sim_loss = 0.561971, global_step = 1725\n",
      "INFO:tensorflow:global_step/sec: 0.849733\n",
      "INFO:tensorflow:\n",
      "****source_ex == a baby sheep and an older sheep in the shade in a field of grass       \n",
      "****target_ex == a baby sheep hanging out with her mother        \n",
      "****predict == a giraffe standing in a field with a giraffe in the background                   \n",
      "INFO:tensorflow:loss = 7.69864, step = 1801 (117.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.989918\n",
      "INFO:tensorflow:\n",
      "****source_ex == a person riding a bike near a traffic light             \n",
      "****target_ex == a street light with a green four leaf clover          \n",
      "****predict == a man sitting on a street       \n",
      "INFO:tensorflow:loss = 6.82954, step = 1901 (101.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.01862\n",
      "INFO:tensorflow:\n",
      "****source_ex == a giraffe is standing with it's head behind another giraffe    \n",
      "****target_ex == two giraffes stand one in front of the other in a pen     \n",
      "****predict == a giraffe standing in a field with a giraffe in the background   \n",
      "INFO:tensorflow:loss = 6.9406, step = 2001 (98.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.02345\n",
      "INFO:tensorflow:\n",
      "****source_ex == a red fire hydrant sitting on the side of a lake       \n",
      "****target_ex == a red fire hydrant sitting beside a lake            \n",
      "****predict == a giraffe standing in a field with a large building   \n",
      "INFO:tensorflow:loss = 5.97443, step = 2101 (97.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.974816\n",
      "INFO:tensorflow:\n",
      "****source_ex == a man with a hat reading a newspaper          \n",
      "****target_ex == a man selling a large variety of veggies        \n",
      "****predict == a man is sitting on a table      \n",
      "INFO:tensorflow:loss = 6.91391, step = 2201 (102.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.994092\n",
      "INFO:tensorflow:\n",
      "****source_ex == a sidewalk is lined with street signs and a fire hydrant     \n",
      "****target_ex == an electrical utility box sitting under a tree next to shrubbery      \n",
      "****predict == a red and white fire hydrant on a street           \n",
      "INFO:tensorflow:loss = 6.98156, step = 2301 (100.594 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2309 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-15:44:42\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-2309\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-15:44:48\n",
      "INFO:tensorflow:Saving dict for global step 2309: global_step = 2309, loss = 8.13122, sim_loss = 0.634869, source_loss = 3.69558, target_loss = 3.80077\n",
      "INFO:tensorflow:Validation (step 2400): source_loss = 3.69558, loss = 8.13122, target_loss = 3.80077, sim_loss = 0.634869, global_step = 2309\n",
      "INFO:tensorflow:global_step/sec: 0.885216\n",
      "INFO:tensorflow:\n",
      "****source_ex == a small bus is seen in a foreign city         \n",
      "****target_ex == a small bus covered in signs drives down a street       \n",
      "****predict == a red fire hydrant on a street                        \n",
      "INFO:tensorflow:loss = 7.33928, step = 2401 (112.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.02126\n",
      "INFO:tensorflow:\n",
      "****source_ex == the mountains are visible beyond the signal lights          \n",
      "****target_ex == the traffic lights and street lights mark webb way         \n",
      "****predict == a large airplane is on the runway        \n",
      "INFO:tensorflow:loss = 7.40359, step = 2501 (97.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.968156\n",
      "INFO:tensorflow:\n",
      "****source_ex == a bullet train sitting in a train station under a sky light       \n",
      "****target_ex == a girl and guy riding horses along the beach          \n",
      "****predict == a train is sitting on the tracks         \n",
      "INFO:tensorflow:loss = 7.20251, step = 2601 (103.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.958215\n",
      "INFO:tensorflow:\n",
      "****source_ex == two women standing on a sidewalk next to a street sign at night while cars drive on the street next to them and behind them \n",
      "****target_ex == two woman near the interstate 15 sign in las vegas       \n",
      "****predict == a street sign on a street with a bus on a street next to a building \n",
      "INFO:tensorflow:loss = 6.98385, step = 2701 (104.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.0056\n",
      "INFO:tensorflow:\n",
      "****source_ex == a street with several advertisements hanging in the street         \n",
      "****target_ex == a man and woman near a girl in a dress     \n",
      "****predict == a street sign with a street sign and a building        \n",
      "INFO:tensorflow:loss = 6.12727, step = 2801 (99.444 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2894 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-15:53:18\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-2894\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-15:53:23\n",
      "INFO:tensorflow:Saving dict for global step 2894: global_step = 2894, loss = 8.18589, sim_loss = 0.770187, source_loss = 3.8026, target_loss = 3.61309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Validation (step 2900): source_loss = 3.8026, loss = 8.18589, target_loss = 3.61309, sim_loss = 0.770187, global_step = 2894\n",
      "INFO:tensorflow:global_step/sec: 0.905278\n",
      "INFO:tensorflow:\n",
      "****source_ex == a train that is on the tracks next to the grass       \n",
      "****target_ex == homemade chicken and curly noodle soup is chock full of carrots corn and peas     \n",
      "****predict == a train is on the tracks near a train    \n",
      "INFO:tensorflow:loss = 6.80607, step = 2901 (110.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.993812\n",
      "INFO:tensorflow:\n",
      "****source_ex == a couple of sheep stand on top of a hill        \n",
      "****target_ex == three animals stand near a fence at the top of a small hill     \n",
      "****predict == a group of sheep standing on a lush green field     \n",
      "INFO:tensorflow:loss = 6.72689, step = 3001 (100.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.982514\n",
      "INFO:tensorflow:\n",
      "****source_ex == narrow city street with vehicles and pedestrian traffic         \n",
      "****target_ex == a view of a city street during the day      \n",
      "****predict == a street sign on a street       \n",
      "INFO:tensorflow:loss = 6.08555, step = 3101 (101.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.01294\n",
      "INFO:tensorflow:\n",
      "****source_ex == a double decker bus by the curve of a driveway           \n",
      "****target_ex == the front of motorcycle that is parked on the grass           \n",
      "****predict == a double decker bus is on the side of a building  \n",
      "INFO:tensorflow:loss = 5.9365, step = 3201 (98.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.03871\n",
      "INFO:tensorflow:\n",
      "****source_ex == a man floating on a boat with a store in it next to a building  \n",
      "****target_ex == clean bathroom with an orange checked shower curtain which is closed      \n",
      "****predict == a man is standing in a parking lot with a truck on a road \n",
      "INFO:tensorflow:loss = 7.27683, step = 3301 (96.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.940177\n",
      "INFO:tensorflow:\n",
      "****source_ex == a stop sign and red light at an intersection at night time       \n",
      "****target_ex == a crowd of people hanging out at the beach         \n",
      "****predict == a stop sign with a stop sign on the street     \n",
      "INFO:tensorflow:loss = 6.66587, step = 3401 (106.363 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3475 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-16:03:37\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-3475\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-16:03:44\n",
      "INFO:tensorflow:Saving dict for global step 3475: global_step = 3475, loss = 7.86867, sim_loss = 0.729504, source_loss = 3.66089, target_loss = 3.47827\n",
      "INFO:tensorflow:Validation (step 3500): source_loss = 3.66089, loss = 7.86867, target_loss = 3.47827, sim_loss = 0.729504, global_step = 3475\n",
      "INFO:tensorflow:global_step/sec: 0.857822\n",
      "INFO:tensorflow:\n",
      "****source_ex == a blue train pulling into a station next to a platform      \n",
      "****target_ex == a passenger train pulled up to an empty train station            \n",
      "****predict == a train is pulling into a train station        \n",
      "INFO:tensorflow:loss = 5.9783, step = 3501 (116.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.00372\n",
      "INFO:tensorflow:\n",
      "****source_ex == a red stop sign sitting on the side of the road        \n",
      "****target_ex == a stop sign is next to a highway near a dessert        \n",
      "****predict == a stop sign on the side of the street          \n",
      "INFO:tensorflow:loss = 5.65464, step = 3601 (99.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.988544\n",
      "INFO:tensorflow:\n",
      "****source_ex == black and white cats laying down in the green grass           \n",
      "****target_ex == a zebra standing next to a metal fence              \n",
      "****predict == a group of cows are standing in the water      \n",
      "INFO:tensorflow:loss = 6.3349, step = 3701 (101.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.989228\n",
      "INFO:tensorflow:\n",
      "****source_ex == large cat standing over another one laying on the ground    \n",
      "****target_ex == a couple of men standing on top of a field    \n",
      "****predict == a cat sitting on top of a body of water      \n",
      "INFO:tensorflow:loss = 6.05927, step = 3801 (101.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.976869\n",
      "INFO:tensorflow:\n",
      "****source_ex == a gray and white cat is lying on a sofa with his front paws tucked in    \n",
      "****target_ex == a cat curled up on a linen colored couch           \n",
      "****predict == a cat is sitting in a chair next to a white bag     \n",
      "INFO:tensorflow:loss = 5.83538, step = 3901 (102.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.00322\n",
      "INFO:tensorflow:\n",
      "****source_ex == trains stopped on the tracks at the railroad at a train station  \n",
      "****target_ex == a long train is coming down some tracks        \n",
      "****predict == train on the train tracks near a train station     \n",
      "INFO:tensorflow:loss = 5.46477, step = 4001 (99.680 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4061 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-16:13:52\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-4061\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-16:13:58\n",
      "INFO:tensorflow:Saving dict for global step 4061: global_step = 4061, loss = 7.33371, sim_loss = 0.64045, source_loss = 3.49166, target_loss = 3.20159\n",
      "INFO:tensorflow:Validation (step 4100): source_loss = 3.49166, loss = 7.33371, target_loss = 3.20159, sim_loss = 0.64045, global_step = 4061\n",
      "INFO:tensorflow:global_step/sec: 0.90567\n",
      "INFO:tensorflow:\n",
      "****source_ex == a woman holds an umbrella as she leaves a store           \n",
      "****target_ex == a bird that is sitting in a tree             \n",
      "****predict == a man is holding a dog in the camera        \n",
      "INFO:tensorflow:loss = 7.57165, step = 4101 (110.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.95611\n",
      "INFO:tensorflow:\n",
      "****source_ex == elephants getting bathed by a man in blue       \n",
      "****target_ex == two elephants that are playing together in the dirt      \n",
      "****predict == two men in a suit and tie        \n",
      "INFO:tensorflow:loss = 6.10309, step = 4201 (104.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.950654\n",
      "INFO:tensorflow:\n",
      "****source_ex == a large group of people holding umbrellas walking down a busy street           \n",
      "****target_ex == people walk along an outdoor market carrying umbrellas in the rain            \n",
      "****predict == a group of people walking a man in the rain       \n",
      "INFO:tensorflow:loss = 5.46349, step = 4301 (105.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.979457\n",
      "INFO:tensorflow:\n",
      "****source_ex == several elephants that are walking together in a zoo       \n",
      "****target_ex == a herd of elephants tramping through the grounds        \n",
      "****predict == two elephants walking in the grass with a group of people      \n",
      "INFO:tensorflow:loss = 5.91775, step = 4401 (102.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.02276\n",
      "INFO:tensorflow:\n",
      "****source_ex == a guy standing there with a black and blue striped tie               \n",
      "****target_ex == an image of a guy that striking a player out      \n",
      "****predict == a couple of luggage with a picture                \n",
      "INFO:tensorflow:loss = 5.48735, step = 4501 (97.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.966684\n",
      "INFO:tensorflow:\n",
      "****source_ex == a senior man in suit and sandals sitting at and playing a grand piano in a ballroom with staircase and balcony with dressy people standing \n",
      "****target_ex == man playing piano at party in hotel ballroom                       \n",
      "****predict == a man in the middle of a suit and tie sitting in front of a small child in front of a cat \n",
      "INFO:tensorflow:loss = 5.9921, step = 4601 (103.446 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4636 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-16:24:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-4636\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-16:24:24\n",
      "INFO:tensorflow:Saving dict for global step 4636: global_step = 4636, loss = 6.8412, sim_loss = 0.793966, source_loss = 3.08148, target_loss = 2.96576\n",
      "INFO:tensorflow:Validation (step 4700): source_loss = 3.08148, loss = 6.8412, target_loss = 2.96576, sim_loss = 0.793966, global_step = 4636\n",
      "INFO:tensorflow:global_step/sec: 0.888462\n",
      "INFO:tensorflow:\n",
      "****source_ex == full perspective of a few creatures and single individual         \n",
      "****target_ex == elephants standing near metal rails with their trunks near the ground        \n",
      "****predict == the young is a picture of a picture of two different items  \n",
      "INFO:tensorflow:loss = 6.06733, step = 4701 (112.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.0048\n",
      "INFO:tensorflow:\n",
      "****source_ex == a black dog pants next to a blue water bottle        \n",
      "****target_ex == a large stainless steel fridge on display in a store setting       \n",
      "****predict == a black bear standing in a field next to a tree    \n",
      "INFO:tensorflow:loss = 5.21963, step = 4801 (99.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.989514\n",
      "INFO:tensorflow:\n",
      "****source_ex == a group of elephants huddle together near large stones             \n",
      "****target_ex == a couple of elephants are near the rocks         \n",
      "****predict == a group of elephants walking around the water        \n",
      "INFO:tensorflow:loss = 5.32088, step = 4901 (101.060 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.00052\n",
      "INFO:tensorflow:\n",
      "****source_ex == some chicks playing ultimate frisbee being gnarly af            \n",
      "****target_ex == an empty bench underneath palm trees in a park           \n",
      "****predict == an elephant is on a bed with a dog           \n",
      "INFO:tensorflow:loss = 5.00367, step = 5001 (99.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.979789\n",
      "INFO:tensorflow:\n",
      "****source_ex == there is a man on the grass with a frisbee       \n",
      "****target_ex == a person is looking out the window in a hotel room      \n",
      "****predict == the man is in a park with the camera       \n",
      "INFO:tensorflow:loss = 4.13637, step = 5101 (102.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.970757\n",
      "INFO:tensorflow:\n",
      "****source_ex == a race horse at work on a green track       \n",
      "****target_ex == a stoplight with cars stopped just after crossing a train track     \n",
      "****predict == a person walking down a beach with a crowd of people       \n",
      "INFO:tensorflow:loss = 4.43952, step = 5201 (103.014 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5216 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-16:34:37\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-5216\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-16:34:43\n",
      "INFO:tensorflow:Saving dict for global step 5216: global_step = 5216, loss = 6.43899, sim_loss = 0.711912, source_loss = 2.97156, target_loss = 2.75552\n",
      "INFO:tensorflow:Validation (step 5300): source_loss = 2.97156, loss = 6.43899, target_loss = 2.75552, sim_loss = 0.711912, global_step = 5216\n",
      "INFO:tensorflow:global_step/sec: 0.877857\n",
      "INFO:tensorflow:\n",
      "****source_ex == a white plate filled with an assortment of fruits        \n",
      "****target_ex == a group of giraffes walking on a barren plain         \n",
      "****predict == a large white plate topped with bananas          \n",
      "INFO:tensorflow:loss = 4.898, step = 5301 (113.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.966957\n",
      "INFO:tensorflow:\n",
      "****source_ex == a man holding a neon yellow frisbee on top of a dirt path    \n",
      "****target_ex == a couple of chairs are sitting by some tables          \n",
      "****predict == a man holding a red frisbee in front of a group of horses    \n",
      "INFO:tensorflow:loss = 5.97235, step = 5401 (103.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.97619\n",
      "INFO:tensorflow:\n",
      "****source_ex == a brown and white dog laying on a bed on the couch           \n",
      "****target_ex == a dog laying on a tiger print fabric on a blue couch   \n",
      "****predict == a black and white dog sitting on a bed                \n",
      "INFO:tensorflow:loss = 3.97116, step = 5501 (102.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.00869\n",
      "INFO:tensorflow:\n",
      "****source_ex == a large brown bear walking across a leaf filled forest       \n",
      "****target_ex == a grizzle bear looking concerned while in the woods      \n",
      "****predict == a large brown bear walking through the middle of a cloudy day     \n",
      "INFO:tensorflow:loss = 3.92588, step = 5601 (99.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.972849\n",
      "INFO:tensorflow:\n",
      "****source_ex == a man that is next to his horse on a hill      \n",
      "****target_ex == a man on elephant crossing body of water with trees in the background \n",
      "****predict == a man that is standing on a horse with frisbees     \n",
      "INFO:tensorflow:loss = 4.42319, step = 5701 (102.792 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5798 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-16:43:17\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-5798\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-16:43:23\n",
      "INFO:tensorflow:Saving dict for global step 5798: global_step = 5798, loss = 5.98371, sim_loss = 0.928212, source_loss = 2.55474, target_loss = 2.50076\n",
      "INFO:tensorflow:Validation (step 5800): source_loss = 2.55474, loss = 5.98371, target_loss = 2.50076, sim_loss = 0.928212, global_step = 5798\n",
      "INFO:tensorflow:global_step/sec: 0.89667\n",
      "INFO:tensorflow:\n",
      "****source_ex == a young man in uniform about to pitch a baseball    \n",
      "****target_ex == a black refrigerator with pat dishes next to it        \n",
      "****predict == a young man eating food while holding a ball    \n",
      "INFO:tensorflow:loss = 4.94159, step = 5801 (111.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.971418\n",
      "INFO:tensorflow:\n",
      "****source_ex == a skateboard crouches down to prepare for a trick            \n",
      "****target_ex == a young man riding a skateboard down a street        \n",
      "****predict == a skateboarder on the snow skateboard at home           \n",
      "INFO:tensorflow:loss = 3.72464, step = 5901 (102.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.972528\n",
      "INFO:tensorflow:\n",
      "****source_ex == a person standing on top of a ski slope wearing skis       \n",
      "****target_ex == a group of young men playing a game of baseball on a field     \n",
      "****predict == a person standing on top of a skateboard with skis          \n",
      "INFO:tensorflow:loss = 3.80504, step = 6001 (102.824 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.994228\n",
      "INFO:tensorflow:\n",
      "****source_ex == the popsicle is flavored with mango and orange         \n",
      "****target_ex == a polar bear as seen from underwater camera       \n",
      "****predict == the two male is filled with various fruits and       \n",
      "INFO:tensorflow:loss = 4.24667, step = 6101 (100.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.01587\n",
      "INFO:tensorflow:\n",
      "****source_ex == two older guys skiing at a ski slope on their skis           \n",
      "****target_ex == a man is cross country skiing on a course with other skiers          \n",
      "****predict == two skiers are skiing down a ski slope with a man on       \n",
      "INFO:tensorflow:loss = 4.65826, step = 6201 (98.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.999582\n",
      "INFO:tensorflow:\n",
      "****source_ex == a display of bananas potatoes and other fruits and vegetables   \n",
      "****target_ex == a man riding on a skateboard down the sidewalk      \n",
      "****predict == a display of bananas that is covered with vegetables and other   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 4.1349, step = 6301 (100.042 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6381 into experiments/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-23-16:53:34\n",
      "INFO:tensorflow:Restoring parameters from experiments/model.ckpt-6381\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-23-16:53:40\n",
      "INFO:tensorflow:Saving dict for global step 6381: global_step = 6381, loss = 5.91707, sim_loss = 1.13424, source_loss = 2.4778, target_loss = 2.30502\n",
      "INFO:tensorflow:Validation (step 6400): source_loss = 2.4778, loss = 5.91707, target_loss = 2.30502, sim_loss = 1.13424, global_step = 6381\n",
      "INFO:tensorflow:global_step/sec: 0.883528\n",
      "INFO:tensorflow:\n",
      "****source_ex == a person is riding a snowboard down a hill in the snow         \n",
      "****target_ex == a man holding a pink kite while standing in a field          \n",
      "****predict == a person is riding a snowboard down a hill in the snow          \n",
      "INFO:tensorflow:loss = 3.79229, step = 6401 (113.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.987465\n",
      "INFO:tensorflow:\n",
      "****source_ex == two women standing on top of a snowy hill     \n",
      "****target_ex == a young woman leaning against a wall with her hands placed on her luggage \n",
      "****predict == two men standing on a mountain filled with skis     \n",
      "INFO:tensorflow:loss = 3.09459, step = 6501 (101.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.977787\n",
      "INFO:tensorflow:\n",
      "****source_ex == pile of mochi an doughnuts non fried on a display      \n",
      "****target_ex == non fried doughnuts in a chinese bakery with english and chinese signage      \n",
      "****predict == there is a small plate on a hill of people       \n",
      "INFO:tensorflow:loss = 3.54199, step = 6601 (102.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.953044\n",
      "INFO:tensorflow:\n",
      "****source_ex == a person snowboarding down a steep hill really quick       \n",
      "****target_ex == a man eating pizza at a restaurant             \n",
      "****predict == a person snowboarding down a hill in the snow         \n",
      "INFO:tensorflow:loss = 3.21016, step = 6701 (104.928 sec)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a very clean and well decorated empty bathroom\r\n",
      "a bathroom with a border of butterflies and blue paint on the walls above it\r\n",
      "a blue and white bathroom with butterfly themed wall tiles\r\n",
      "an angled view of a beautifully decorated bathroom\r\n",
      "a panoramic view of a kitchen and all of its appliances\r\n",
      "a wide angle view of the kitchen work area\r\n",
      "a panoramic photo of a kitchen and dining room\r\n",
      "multiple photos of a brown and white kitchen\r\n",
      "a graffiti ed stop sign across the street from a red car\r\n",
      "a red stop sign with a bush bumper sticker under the word stop\r\n"
     ]
    }
   ],
   "source": [
    "!head data/mscoco/train_source.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a black metal bicycle with a clock inside the front wheel\r\n",
      "a bicycle replica with a clock as the front wheel\r\n",
      "a bicycle figurine in which the front wheel is replaced with a clock\r\n",
      "the bike has a clock as a tire\r\n",
      "a black honda motorcycle with a dark burgundy seat\r\n",
      "a black honda motorcycle parked in front of a garage\r\n",
      "ma motorcycle parked on the gravel in front of a garage\r\n",
      "a honda motorcycle parked in a grass driveway\r\n",
      "this is a blue and white bathroom with a wall sink and a lifesaver on the wall\r\n",
      "a room with blue walls and a white sink and door\r\n"
     ]
    }
   ],
   "source": [
    "!head data/mscoco/test_target.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a metal clock tower on a sidewalk in front of a very tall building\r\n",
      "a group of boats on top of wet sand\r\n",
      "a kitchen with wood floors and  green counter tops\r\n",
      "a slice of pizza sitting on top of a paper plate\r\n",
      "a man with safety equipment next to a fallen tree and red fire hydrant\r\n",
      "three people are riding on an elephant's back through the jungle\r\n",
      "a teddy bear resting on a pillow in a child's bedroom\r\n",
      "a couple of people that are on their cell phone\r\n",
      "a man wearing a purple shirt  black vest and tie posing for the camera\r\n",
      "a person skating in very much snow with warm clothes\r\n"
     ]
    }
   ],
   "source": [
    "!head data/mscoco/test_target_shuffled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'data/mscoco/test_target_shuffled.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1, file2, file3 = 'data/mscoco/test_target_shuffled.txt', 'data/mscoco/test_target_shuffled.txt', 'data/mscoco/test_target_shuffled.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1, file2, file3 = (re.sub('test', 'train', f) for f in (file1, file2, file3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/mscoco/train_target_shuffled.txt'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/mscoco/train_target_shuffled.txt'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/mscoco/train_target_shuffled.txt'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/mscoco/test_target_shuffled.txt'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('train', 'test', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_source.txt            train_target.txt\r\n",
      "test_target.txt            train_target_shuffled.txt\r\n",
      "test_target_shuffled.txt   train_vocab.txt\r\n",
      "train_source.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls data/mscoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = open('data/mscoco/train_source.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12811]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i, sentence in enumerate(train_source) if sentence == 'young people and their groceries in a kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a cooking range is sitting in the middle of the kitchen',\n",
       " 'a group of pretty ladies and one man standing around a table',\n",
       " 'several people standing around a table with bags on it',\n",
       " 'woman and man around kitchen island with grocery bags and beverages',\n",
       " 'young people and their groceries in a kitchen',\n",
       " 'two dogs are sitting in a cart attached to a parked bicycle']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source[12807:12813]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = open('data/mscoco/train_target.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a stove is away from the wall in a kitchen area',\n",
       " 'several people standing around a table with bags on it',\n",
       " 'a group of pretty ladies and one man standing around a table',\n",
       " 'young people and their groceries in a kitchen',\n",
       " 'woman and man around kitchen island with grocery bags and beverages',\n",
       " 'a bicycle leaning against a wall with a seat attachment behind it and two dogs sitting in the attachment']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[12807:12813]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source = open('data/mscoco/test_source.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = open('data/mscoco/test_target.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162024"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(test_target)\n",
    "with open('data/mscoco/test_target_shuffled.txt', 'w') as f:\n",
    "    for line in test_target:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bicycle replica with a clock as the front wheel\n",
      "a black metal bicycle with a clock inside the front wheel\n",
      "\n",
      "\n",
      "a black metal bicycle with a clock inside the front wheel\n",
      "a bicycle replica with a clock as the front wheel\n",
      "\n",
      "\n",
      "the bike has a clock as a tire\n",
      "a bicycle figurine in which the front wheel is replaced with a clock\n",
      "\n",
      "\n",
      "a bicycle figurine in which the front wheel is replaced with a clock\n",
      "the bike has a clock as a tire\n",
      "\n",
      "\n",
      "a black honda motorcycle parked in front of a garage\n",
      "a black honda motorcycle with a dark burgundy seat\n",
      "\n",
      "\n",
      "a black honda motorcycle with a dark burgundy seat\n",
      "a black honda motorcycle parked in front of a garage\n",
      "\n",
      "\n",
      "a honda motorcycle parked in a grass driveway\n",
      "ma motorcycle parked on the gravel in front of a garage\n",
      "\n",
      "\n",
      "ma motorcycle parked on the gravel in front of a garage\n",
      "a honda motorcycle parked in a grass driveway\n",
      "\n",
      "\n",
      "a room with blue walls and a white sink and door\n",
      "this is a blue and white bathroom with a wall sink and a lifesaver on the wall\n",
      "\n",
      "\n",
      "this is a blue and white bathroom with a wall sink and a lifesaver on the wall\n",
      "a room with blue walls and a white sink and door\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print test_source[i]\n",
    "    print test_target[i]\n",
    "    print '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handler import Data\n",
    "\n",
    "data = Data(FLAGS)\n",
    "input_fn, feed_fn = data.make_input_fn()\n",
    "features, _ = input_fn()\n",
    "feed = feed_fn()\n",
    "\n",
    "model = Seq2seq(data.vocab_size, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_source, loss, source, target, label = model.make_graph(\n",
    "        tf.estimator.ModeKeys.TRAIN, features, None, FLAGS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sess.run(train_output_source.rnn_output, feed_dict={source: feed['source:0'],\n",
    "                                  target: feed['target:0'],\n",
    "                                  label: feed['label:0']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = sess.run(source, feed_dict={source: feed['source:0'],\n",
    "                                  target: feed['target:0'],\n",
    "                                  label: feed['label:0']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16, 22946)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = feed_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_shape = out.shape[1]\n",
    "lab_shape = actual.shape[1]\n",
    "while seq_shape == lab_shape:\n",
    "    feed = feed_fn()\n",
    "    out = sess.run(train_output_source.rnn_output, feed_dict={source: feed['source:0'],\n",
    "                                  target: feed['target:0'],\n",
    "                                  label: feed['label:0']})\n",
    "    actual = sess.run(source, feed_dict={source: feed['source:0'],\n",
    "                                  target: feed['target:0'],\n",
    "                                  label: feed['label:0']})\n",
    "    seq_shape = out.shape[1]\n",
    "    lab_shape = actual.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 30, 22946)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.LSTMCell(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.contrib.seq2seq.BasicDecoder(cell, helper=helper, initial_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicDecoderOutput(rnn_output=256, sample_id=TensorShape([]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19359  7825 19450 22224 14156 14984  2327 21692     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 21692   718 19359  8345 10744   917 22224  4184  9458  1656  7361\n",
      "  9413  7133  8775     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359  4184 22224 21969 21692   718 17692 20310 17065 14465     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[ 6335 21313 22784 10744 19359 18142 14984 21692     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 10868 22784 10744 19359  1625 22224 16487 10744 17988 22253     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 21974 17008 22784 10744  7361  1625 18667 19253     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 10868 13155 10744 19359  1625 22224 10570  2824     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[21989   616 10744 19359 18981 22224 21969  1625     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359   909  7558  8714 11826 20513  7361 10057  5287 19359 12895 13301\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 12895  8714 11826   718 19359 17370  6487 18865 15756  7361 18664\n",
      "  8714     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 20352  8714 11826 22224 19359 12895 11916  1656  7361 20651     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359  8714 11826 19111   288 10284 20352  8773  2305  8780 19706 10744\n",
      " 19359  8364 13301     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[ 7361 15052  5924 19754  6166  3083  7361 12943     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[15052  5392  6340 19359 21969  7070 12943   718 17747     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[15052  5924 18323  4681 15698  1656 19359 12943     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359  1018  6340  7361 12943  6166   718 16994  4681 15698     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 21100 22224 19359 22171 11767 19359   771 21692     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19450 19789 21692   718 14471 17499 22224 10661  4962     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[21969 12132 21100 22224 22171  8249  8780 19359 15751 19433 21692     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 21692   718 22171 22224 21100 22224  4184 17065     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[ 6335  2327  1625   718 21969 22224  9115 22253     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[  771  1625  8780 19359  2863  2695   718 15722  7137     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 20878 22224 17027 19754  8780 19359   771  1625 19253     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359   771  1625   718 21100 17027 22224 20878     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 21969 11915  1625   718  8277 17499 19111  3562 15218     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359  1625   718 19359  8277 17499   288  2098   718 10390 11620 19359\n",
      "  2228 19359 21100 22224 19359 20878     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 21969  1625  8780 19359  2695   718  7361 16933  1656     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 21969  1625  4777 11620  1656  7361  2098     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[ 7361 20253 13644 15052  7137   718 19359  4796  5930  3424     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[ 7361 21692 12602  8773 12517   718 10275 22224  4191     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[19359 19450  3341   718 10275 22224  9883  4283  5357     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n",
      "[ 4834  8773 19359 19450 21692 12602 22224 21100     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1]\n"
     ]
    }
   ],
   "source": [
    "for a in actual:\n",
    "    print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 27)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 26)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 19, 22946)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19359,  7825, 19450, 22224, 14156, 14984,  2327, 21692,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source\n",
      "target\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "for op in  sess.graph.get_operations():\n",
    "    if re.search('laceholder', op.type):\n",
    "        print op.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicDecoderOutput(rnn_output=<tf.Tensor 'decode/decoder/transpose:0' shape=(?, ?, 22946) dtype=float32>, sample_id=<tf.Tensor 'decode/decoder/transpose_1:0' shape=(?, ?) dtype=int32>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import layers\n",
    "\n",
    "def encode(seq, reuse=None):\n",
    "    input_lengths  = tf.reduce_sum(tf.to_int32(tf.not_equal(seq, 1)), 1)\n",
    "    input_embed    = layers.embed_sequence(seq,\n",
    "                                           vocab_size=vocab_size,\n",
    "                                           embed_dim =embed_dim,\n",
    "                                           scope = 'embed',\n",
    "                                           reuse = reuse)\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=num_units, reuse=reuse)\n",
    "    if FLAGS.use_residual_lstm:\n",
    "        cell = tf.contrib.rnn.ResidualWrapper(cell)\n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell, input_embed, dtype=tf.float32)\n",
    "#     encoder_final_state = tf.concat(encoder_final_state, 1)\n",
    "    return encoder_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encoder_out, scope, output=None, mode='train', reuse=None):\n",
    "\n",
    "    # From the encoder\n",
    "    encoder_outputs = encoder_out[0]\n",
    "    encoder_state = encoder_out[1]\n",
    "    input_lengths = encoder_out[2]\n",
    "\n",
    "    # Perform the embedding\n",
    "    if mode=='train':\n",
    "        if output is None:\n",
    "            raise Exception('output must be provided for mode=train')\n",
    "        train_output   = tf.concat([tf.expand_dims(start_tokens, 1), output], 1)\n",
    "        output_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(train_output, 1)), 1)\n",
    "        output_embed   = layers.embed_sequence(\n",
    "            train_output,\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim = embed_dim,\n",
    "            scope = 'encode/embed', reuse = True)\n",
    "\n",
    "    # Prepare the helper\n",
    "    if mode=='train':\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(output_embed, output_lengths)\n",
    "    if mode=='predict':\n",
    "        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embeddings,\n",
    "            start_tokens=tf.to_int32(start_tokens),\n",
    "            end_token=1\n",
    "            )\n",
    "        \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_units=num_units)\n",
    "        out_cell = tf.contrib.rnn.OutputProjectionWrapper(cell, vocab_size, reuse=reuse)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=out_cell, helper=helper,\n",
    "            initial_state=encoder_state\n",
    "            )\n",
    "        outputs = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder=decoder, output_time_major=False,\n",
    "            impute_finished=True, maximum_iterations=FLAGS.output_max_length)\n",
    "\n",
    "        return outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_loss(self, decoding, actual):\n",
    "        train_output = tf.concat([tf.expand_dims(self.start_tokens, 1), actual], 1)\n",
    "        weights = tf.to_float(tf.not_equal(train_output[:, :-1], 1))\n",
    "        # tf.identity(decoding.rnn_output[0], name='decoder_output')\n",
    "        # tf.identity(actual[0], name='actual')\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(decoding.rnn_output,\n",
    "                                                actual,\n",
    "                                                # average_across_timesteps=True,\n",
    "                                                # average_across_batch=True,\n",
    "                                                weights=weights)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = params.embed_dim\n",
    "num_units = params.num_units\n",
    "\n",
    "# Data\n",
    "source, target, label   = features['source'], features['target'], features['label']\n",
    "batch_size     = tf.shape(source)[0]\n",
    "start_tokens   = tf.zeros([batch_size], dtype= tf.int64)\n",
    "\n",
    "with tf.variable_scope('encode'):\n",
    "    source_encoder_out = encode(source)\n",
    "    target_encoder_out = encode(target, reuse=True)\n",
    "\n",
    "# Save embeddings\n",
    "with tf.variable_scope('encode/embed', reuse=True):\n",
    "    embeddings = tf.get_variable('embeddings')\n",
    "\n",
    "# From the encoder\n",
    "# encoder_state = source_encoder_out[1]\n",
    "\n",
    "train_output   = tf.concat([tf.expand_dims(start_tokens, 1), source], 1)\n",
    "output_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(train_output, 1)), 1)\n",
    "output_embed   = layers.embed_sequence(\n",
    "    train_output,\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim = embed_dim,\n",
    "    scope = 'encode/embed', reuse = True)\n",
    "\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(output_embed, output_lengths)\n",
    "with tf.variable_scope('decode', reuse=None):\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=num_units)\n",
    "    out_cell = tf.contrib.rnn.OutputProjectionWrapper(cell, vocab_size, reuse=None)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=out_cell, helper=helper,\n",
    "        initial_state=source_encoder_out\n",
    "        )\n",
    "\n",
    "train_output_source = decode(source_encoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder=decoder, output_time_major=False,\n",
    "    impute_finished=True, maximum_iterations=FLAGS.output_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode/embed/embeddings\n",
      "encode/rnn/lstm_cell/kernel\n",
      "encode/rnn/lstm_cell/bias\n",
      "decoder/output_projection_wrapper/lstm_cell/kernel\n",
      "decoder/output_projection_wrapper/lstm_cell/bias\n",
      "decoder/output_projection_wrapper/kernel\n",
      "decoder/output_projection_wrapper/bias\n"
     ]
    }
   ],
   "source": [
    "for op in  sess.graph.get_operations():\n",
    "    if re.search('Variable', op.type):\n",
    "        print op.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
